{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import keras.callbacks as kc\n",
    "import keras.optimizers as ko\n",
    "import keras.utils as ku\n",
    "import numpy as np\n",
    "import sklearn.utils as sku\n",
    "\n",
    "import Config as conf\n",
    "import CSV as csv\n",
    "from DenseNet import DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Input directory not found. Calculate merged CSVs...\n",
      "Create window of try_7 ...\n",
      "Processing X:  ./Dataset/csi_try_7_001.csv\n",
      "Processing X:  ./Dataset/csi_try_7_002.csv\n",
      "Processing X:  ./Dataset/csi_try_7_003.csv\n",
      "Processing X:  ./Dataset/csi_try_7_004.csv\n",
      "Processing X:  ./Dataset/csi_try_7_005.csv\n",
      "Processing X:  ./Dataset/csi_try_7_006.csv\n",
      "Processing X:  ./Dataset/csi_try_7_007.csv\n",
      "Processing X:  ./Dataset/csi_try_7_008.csv\n",
      "Saving X of 200 windows\n",
      "Processing X:  ./Dataset/csi_try_7_009.csv\n",
      "Processing X:  ./Dataset/csi_try_7_010.csv\n",
      "Processing X:  ./Dataset/csi_try_7_011.csv\n",
      "Processing Y:  ./Dataset/action_try_7_001.csv\n",
      "Processing Y:  ./Dataset/action_try_7_002.csv\n",
      "Processing Y:  ./Dataset/action_try_7_003.csv\n",
      "Processing Y:  ./Dataset/action_try_7_004.csv\n",
      "Processing Y:  ./Dataset/action_try_7_005.csv\n",
      "Processing Y:  ./Dataset/action_try_7_006.csv\n",
      "Processing Y:  ./Dataset/action_try_7_007.csv\n",
      "Processing Y:  ./Dataset/action_try_7_008.csv\n",
      "Saving Y of 200 windows\n",
      "Processing Y:  ./Dataset/action_try_7_009.csv\n",
      "Processing Y:  ./Dataset/action_try_7_010.csv\n",
      "Processing Y:  ./Dataset/action_try_7_011.csv\n",
      "Create window of try_2 ...\n",
      "Processing X:  ./Dataset/csi_try_2_001.csv\n",
      "Processing X:  ./Dataset/csi_try_2_002.csv\n",
      "Processing X:  ./Dataset/csi_try_2_003.csv\n",
      "Processing X:  ./Dataset/csi_try_2_004.csv\n",
      "Processing X:  ./Dataset/csi_try_2_005.csv\n",
      "Processing X:  ./Dataset/csi_try_2_006.csv\n",
      "Saving X of 200 windows\n",
      "Processing X:  ./Dataset/csi_try_2_007.csv\n",
      "Processing X:  ./Dataset/csi_try_2_008.csv\n",
      "Processing X:  ./Dataset/csi_try_2_009.csv\n",
      "Processing X:  ./Dataset/csi_try_2_010.csv\n",
      "Processing X:  ./Dataset/csi_try_2_011.csv\n",
      "Processing X:  ./Dataset/csi_try_2_012.csv\n",
      "Processing X:  ./Dataset/csi_try_2_013.csv\n",
      "Processing X:  ./Dataset/csi_try_2_014.csv\n",
      "Saving X of 200 windows\n",
      "Processing X:  ./Dataset/csi_try_2_015.csv\n",
      "Processing X:  ./Dataset/csi_try_2_016.csv\n",
      "Processing X:  ./Dataset/csi_try_2_017.csv\n",
      "Processing X:  ./Dataset/csi_try_2_018.csv\n",
      "Processing X:  ./Dataset/csi_try_2_019.csv\n",
      "Processing X:  ./Dataset/csi_try_2_020.csv\n",
      "Processing X:  ./Dataset/csi_try_2_021.csv\n",
      "Saving X of 200 windows\n",
      "Processing X:  ./Dataset/csi_try_2_022.csv\n",
      "Processing X:  ./Dataset/csi_try_2_023.csv\n",
      "Processing X:  ./Dataset/csi_try_2_024.csv\n",
      "Processing X:  ./Dataset/csi_try_2_025.csv\n",
      "Processing X:  ./Dataset/csi_try_2_026.csv\n",
      "Processing X:  ./Dataset/csi_try_2_027.csv\n",
      "Processing X:  ./Dataset/csi_try_2_028.csv\n",
      "Processing X:  ./Dataset/csi_try_2_029.csv\n",
      "Saving X of 200 windows\n",
      "Processing X:  ./Dataset/csi_try_2_030.csv\n",
      "Processing X:  ./Dataset/csi_try_2_031.csv\n",
      "Processing X:  ./Dataset/csi_try_2_032.csv\n",
      "Processing X:  ./Dataset/csi_try_2_033.csv\n",
      "Processing X:  ./Dataset/csi_try_2_034.csv\n",
      "Processing X:  ./Dataset/csi_try_2_035.csv\n",
      "Processing X:  ./Dataset/csi_try_2_036.csv\n",
      "Saving X of 200 windows\n",
      "Processing X:  ./Dataset/csi_try_2_037.csv\n",
      "Processing X:  ./Dataset/csi_try_2_038.csv\n",
      "Processing X:  ./Dataset/csi_try_2_039.csv\n",
      "Processing X:  ./Dataset/csi_try_2_040.csv\n",
      "Processing X:  ./Dataset/csi_try_2_041.csv\n",
      "Processing X:  ./Dataset/csi_try_2_042.csv\n",
      "Processing X:  ./Dataset/csi_try_2_043.csv\n",
      "Processing X:  ./Dataset/csi_try_2_044.csv\n",
      "Saving X of 200 windows\n",
      "Processing X:  ./Dataset/csi_try_2_045.csv\n",
      "Processing X:  ./Dataset/csi_try_2_046.csv\n",
      "Processing X:  ./Dataset/csi_try_2_047.csv\n",
      "Processing X:  ./Dataset/csi_try_2_048.csv\n",
      "Processing X:  ./Dataset/csi_try_2_049.csv\n",
      "Processing X:  ./Dataset/csi_try_2_050.csv\n",
      "Processing X:  ./Dataset/csi_try_2_051.csv\n",
      "Saving X of 200 windows\n",
      "Processing X:  ./Dataset/csi_try_2_052.csv\n",
      "Processing X:  ./Dataset/csi_try_2_053.csv\n",
      "Processing X:  ./Dataset/csi_try_2_054.csv\n",
      "Processing X:  ./Dataset/csi_try_2_055.csv\n",
      "Processing X:  ./Dataset/csi_try_2_056.csv\n",
      "Processing X:  ./Dataset/csi_try_2_057.csv\n",
      "Processing X:  ./Dataset/csi_try_2_058.csv\n",
      "Processing X:  ./Dataset/csi_try_2_059.csv\n",
      "Saving X of 200 windows\n",
      "Processing X:  ./Dataset/csi_try_2_060.csv\n",
      "Processing X:  ./Dataset/csi_try_2_061.csv\n"
     ]
    }
   ],
   "source": [
    "# Import & shuffle CSV data\n",
    "xx, yy = csv.getCSV()\n",
    "xx, yy = sku.shuffle(xx, yy, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Keras DenseNet Model (DenseNet-BC)\n",
    "nadam = ko.Nadam(lr=conf.LEARNING_RATE)\n",
    "model = DenseNet(\n",
    "    input_shape=(conf.WINDOW_SIZE, conf.N_COLUMNS, 1),\n",
    "    dense_blocks=5,\n",
    "    growth_rate=conf.N_FILTERS,\n",
    "    nb_classes=conf.N_VALID_CLASSES,\n",
    "    dropout_rate=0.2,\n",
    "    bottleneck=True,\n",
    "    compression=0.5,\n",
    "    weight_decay=1e-4,\n",
    "    depth=conf.DEPTH\n",
    ").build_model()\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=nadam, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output directory and prepare tensorboard\n",
    "outputDir = conf.OUTPUT_PATH.format(conf.LEARNING_RATE, conf.BATCH_SIZE,\n",
    "                                    conf.N_FILTERS, \"\")\n",
    "if os.path.exists(outputDir):\n",
    "    shutil.rmtree(outputDir)\n",
    "os.makedirs(outputDir)\n",
    "logDir = conf.LOG_PATH.format(conf.LEARNING_RATE, conf.BATCH_SIZE,\n",
    "                              conf.N_FILTERS, \"\")\n",
    "if os.path.exists(logDir):\n",
    "    shutil.rmtree(logDir)\n",
    "os.makedirs(logDir)\n",
    "tensorboard = kc.TensorBoard(\n",
    "    log_dir=logDir,\n",
    "    histogram_freq=0,\n",
    "    batch_size=conf.BATCH_SIZE,\n",
    "    write_graph=True,\n",
    "    write_grads=True,\n",
    "    write_images=True,\n",
    "    update_freq=10)\n",
    "print(\n",
    "    \"If you have tensorboard in this environment, you can type below to see the result in tensorboard:\"\n",
    ")\n",
    "print(\"    tensorboard --logdir=\" + logDir)\n",
    "print(\"Keras checkpoints and final result will be saved in here:\")\n",
    "print(\"    \" + outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KFold\n",
    "for i in range(conf.KFOLD):\n",
    "    # Roll the data\n",
    "    xx = np.roll(xx, int(len(xx) / conf.KFOLD), axis=0)\n",
    "    yy = np.roll(yy, int(len(yy) / conf.KFOLD), axis=0)\n",
    "\n",
    "    # Data separation\n",
    "    xTrain = xx[int(len(xx) / conf.KFOLD):]\n",
    "    yTrain = yy[int(len(yy) / conf.KFOLD):]\n",
    "    xEval = xx[:int(len(xx) / conf.KFOLD)]\n",
    "    yEval = yy[:int(len(yy) / conf.KFOLD)]\n",
    "\n",
    "    if not conf.USE_NOACTIVITY:\n",
    "        # Remove NoActivity from ys\n",
    "        yTrain = yTrain[:, 1:]\n",
    "        yEval = yEval[:, 1:]\n",
    "\n",
    "        # If there exists only one action, convert Y to binary form\n",
    "        if yEval.shape[1] == 1:\n",
    "            yTrain = ku.to_categorical(yTrain)\n",
    "            yEval = ku.to_categorical(yEval)\n",
    "\n",
    "    # Setup Keras Checkpoint\n",
    "    checkpoint = kc.ModelCheckpoint(\n",
    "        outputDir + \"Checkpoint_K\" + str(i + 1) + \"_EPOCH{epoch}_ACC{val_acc:.6f}.h5\", period=conf.CP_PERIOD)\n",
    "\n",
    "    # Fit model (learn)\n",
    "    print(\n",
    "        str(i + 1) + \" th fitting started. Endpoint is \" + str(conf.KFOLD) +\n",
    "        \" th.\")\n",
    "\n",
    "    model.fit(\n",
    "        xTrain,\n",
    "        yTrain,\n",
    "        batch_size=conf.BATCH_SIZE,\n",
    "        epochs=conf.N_EPOCH,\n",
    "        verbose=1,\n",
    "        callbacks=[tensorboard, checkpoint],\n",
    "        validation_data=(xEval, yEval))  # , validation_freq=2)\n",
    "print(\"Epoch completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "print(\"Saving model & model information...\")\n",
    "modelYML = model.to_yaml()\n",
    "with open(outputDir + \"model.yml\", \"w\") as yml:\n",
    "    yml.write(modelYML)\n",
    "modelJSON = model.to_json()\n",
    "with open(outputDir + \"model.json\", \"w\") as json:\n",
    "    json.write(modelJSON)\n",
    "model.save(outputDir + \"model.h5\")\n",
    "print('Model saved! Congratulations! You finished all processes of ML!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finished"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
